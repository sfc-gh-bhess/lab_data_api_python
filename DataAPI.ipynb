{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "73pvtveipegysnr7heta",
   "authorId": "2448766293310",
   "authorName": "USER",
   "authorEmail": "",
   "sessionId": "2e3b1aa2-0c87-4066-b724-e7bd988ee7f3",
   "lastEditTime": 1748025908123
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c54d9db-d9ed-4391-a31c-74240c6314e4",
   "metadata": {
    "name": "Introduction",
    "collapsed": false
   },
   "source": "# Build a Custom API in Python and Flask\n\n## 1. Overview \n\nMany builders want to share some of the data stored in Snowflake over an http API. Modern mobile and web applications often want to retrieve that data through http APIs. This tutorial will go through how to build, deploy, and host a custom API Powered by Snowflake.\n\nThis API consists of reporting endpoints from data stored in Snowflake. After completing this guide, you will have built a custom API built with [Python Flask](https://flask.palletsprojects.com/). \n\nThe dataset is the [TPC-H](https://docs.snowflake.com/en/user-guide/sample-data-tpch) data set included in your Snowflake account.\n\n\n### Prerequisites\n- Privileges necessary to create a user, database, warehouse, compute pool, repository, network network rule, external access integration, and service in Snowflake\n- Privileges necessary to access the tables in the `SNOWFLAKE_SAMPLE_DATA.TPCH_SF10` database and schema\n- Access to run SQL in the Snowflake console or SnowSQL\n- Basic experience using git, GitHub, and Codespaces\n- Intermediate knowledge of Python\n\n### What You’ll Learn \n- How to configure and build a custom API Powered by Snowflake\n- How to build, publish, and deploy a container with the API in Snowflake\n\n### What You’ll Need \n- [Snowflake](https://snowflake.com) Account in an AWS commercial region\n- [GitHub](https://github.com/) Account with credits for Codespaces\n\n### What You’ll Build \n- API Powered by Snowflake"
  },
  {
   "cell_type": "markdown",
   "id": "4400f7b4-39e0-41fd-90db-ac92d914e15e",
   "metadata": {
    "name": "Setting_up_your_Development_Environment",
    "collapsed": false
   },
   "source": "---\n## 2. Setting up your Development Environment\n\nThe code used in this guide is hosted in github. You will need a new Codespace from the GitHub [repository](https://github.com/sfc-gh-bhess/lab_data_api_python).\n\nTo create a new codespace, browse to the GitHub [repository](https://github.com/sfc-gh-bhess/lab_data_api_python) in a browser. \nYou will need to login to GitHub if you are not already logged in to access Codespaces. After logging in, click on the green \"<> Code\" button and \"create codespace on main\" button.\n\nYou will then be redirected into Codespaces where your development environment will load and all code from GitHub will be loaded in the project. \n\nLet's take a quick look at the code in this repository.\n\n### Endpoints\nThe API creates two sets of endpoints, one for using the Snowflake connector:\n1. `https://host/connector/customers/top10`\n  * Which takes the following optional query parameters:\n    1. `start_range` - the start date of the range in `YYYY-MM-DD` format. Defaults to `1995-01-01`.\n    1. `end_range` - the end date of the range in `YYYY-MM-DD` format. Defaults to `1995-03-31`.\n2. `https://host/connector/clerk/CLERK_ID/yearly_sales/YEAR`\n  * Which takes 2 required path parameters:\n    1. `CLERK_ID` - the clerk ID. Use just the numbers, such as `000000001`.\n    2. `YEAR` - the year to use, such as `1995`.\n\nAnd the same ones using Snowpark:\n1. `https://host/snowpark/customers/top10`\n  * Which takes the following optional query parameters:\n    1. `start_range` - the start date of the range in `YYYY-MM-DD` format. Defaults to `1995-01-01`.\n    1. `end_range` - the end date of the range in `YYYY-MM-DD` format. Defaults to `1995-03-31`.\n2. `https://host/snowpark/clerk/CLERK_ID/yearly_sales/YEAR`\n  * Which takes 2 required path parameters:\n    1. `CLERK_ID` - the clerk ID. Use just the numbers, such as `000000001`.\n    2. `YEAR` - the year to use, such as `1995`.\n\n### Code\nThe `src/` directory has all the source code for the API. The `connector.py` file contains all the entrypoints for the API endpoints using the Snowflake Connector for Python. \nThe `customers_top10()` function is one of the API endpoints we needed for this API which finds the top 10 customers by sales in a date range. \nReview the code and the SQL needed to retrieve the data from Snowflake and serialize it to JSON for the response. \nThis endpoint also takes 2 optional query string parameters start_range and end_range.\n\n```python\n@connector.route('/customers/top10')\ndef customers_top10():\n    # Validate arguments\n    sdt_str = request.args.get('start_range') or '1995-01-01'\n    edt_str = request.args.get('end_range') or '1995-03-31'\n    try:\n        sdt = datetime.datetime.strptime(sdt_str, dateformat)\n        edt = datetime.datetime.strptime(edt_str, dateformat)\n    except:\n        abort(400, \"Invalid start and/or end dates.\")\n    sql_string = '''\n        SELECT\n            o_custkey\n          , SUM(o_totalprice) AS sum_totalprice\n        FROM snowflake_sample_data.tpch_sf10.orders\n        WHERE o_orderdate >= '{sdt}'\n          AND o_orderdate <= '{edt}'\n        GROUP BY o_custkey\n        ORDER BY sum_totalprice DESC\n        LIMIT 10\n    '''\n    sql = sql_string.format(sdt=sdt, edt=edt)\n    try:\n        res = conn.cursor(DictCursor).execute(sql)\n        return make_response(jsonify(res.fetchall()))\n    except:\n        abort(500, \"Error reading from Snowflake. Check the logs for details.\")\n```\n\nYou can also review the other endpoints in [connector.py](https://github.com/sfc-gh-bhess/lab_data_api_python/blob/main/src/connector.py) to \nsee how simple it is to host multiple endpoints.\n\nIf you would also like to see how to build endpoints using the Snowflake Snowpark API, \nreview [snowpark.py](https://github.com/sfc-gh-bhess/lab_data_api_python/blob/main/src/snowpark.py)."
  },
  {
   "cell_type": "markdown",
   "id": "1201326a-2cd2-4869-bebb-acfed3a92fb0",
   "metadata": {
    "name": "Setting_up_Snowflake_CLI",
    "collapsed": false
   },
   "source": "---\n## 3. Setting up Snowflake CLI\n\nAll of the commands in this step will be run in the terminal in Codespaces.\n\nFirst, we need to install Snowflake CLI, with the following command in the terminal:\n```bash\npip install snowflake-cli\n```\n\nNext, we will create a connection for SnowCLI to our Snowflake account. When you\nrun the following command in a terminal, you will be prompted for various details. \nYou only need to supply a connection name (use `my_snowflake`), the user name, and\nthe password. The other prompts are optional:\n\n```bash\nsnow connection add\n```\n\nTo make this connection the default connection that SnowCLI uses, run the following in the terminal:\n\n```bash\nsnow connection set-default my_snowflake\n```\n\nTest that the connection is properly set up by running the following in a terminal:\n\n```bash\nsnow connection test\n```\n\nLet's create a database for this lab using Snowflake CLI. Run the following command in a terminal:\n\n```bash\nsnow object create database name=api\n```"
  },
  {
   "cell_type": "markdown",
   "id": "0074de90-d006-49b0-b5ea-87945611aa82",
   "metadata": {
    "name": "Creating_a_Notebook",
    "collapsed": false
   },
   "source": "---\n## 4. Creating a Notebook for this Lab\n\nIt is useful to use a Notebook to follow the steps for this lab. It allows multiple commands to be put in a single cell and executed, and it allows\nseeing the output of previous steps.\n\nYou can create a new Notebook in Snowflake and copy commands from this guide into new cells and execute them. Alternatively, the repo for this\nlab comes with a Notebook file you can use to create a Notebook in Snowflake with all of the commands in it.\n\n### Importing the Notebook file\nTo create a Notebook with this lab and commands in it, first download the `DataAPI.ipynb` file from the lab repository, \n[here](https://raw.githubusercontent.com/sfc-gh-bhess/lab_data_api_python/refs/heads/main/DataAPI.ipynb). \nIf you are using Codespaces, you can right-click on the file in the file explorer and choose \"Download\".\n\nNext, in the Snowflake console, choose the \"Projects\" sidebar and select \"Notebooks\". Choose the down arrow next to the \"+ Notebook\"\nbutton and select \"Import .ipynb file\". You will be prompted to choose the file from your machine - choose the `DataAPI.ipynb` file that you saved.\nNext, you will be shown a form to collect information about your Notebook. You can choose any name you would like (e.g., `Data API`). \nChoose the `API` database and the `PUBLIC` schema. Choose \"Run on warehouse\". Leave all of the other inputs with their defaults.\n\nWhen the Notebook is created, click the \"Start\" button on the top.\n\n### Creating an empty Notebook\nTo create an empty Notebook, in the Snowflake console, choose the \"Projects\" sidebar and select \"Notebooks\". Click the \"+ Notebook\" button\nin the top left. Next, you will be shown a form to collect information about your Notebook. You can choose any name you would like (e.g., `Data API`). \nChoose the `API` database and the `PUBLIC` schema. Choose \"Run on warehouse\". Leave all of the other inputs with their defaults.\n\nWhen the Notebook is created, click the \"Start\" button on the top.\n\n"
  },
  {
   "cell_type": "markdown",
   "id": "ff51e81d-bbb7-4226-9841-58aef1715129",
   "metadata": {
    "name": "Setting_up_a_Database_and_Warehouse",
    "collapsed": false
   },
   "source": "---\n## 5. Setting up a Database and Warehouse\n\nThe API needs a warehouse to query the data to return to the caller. To create the database and warehouse, \nrun the following commands in the Snowflake (in a cell in a Snowflake Notebook, in a Worksheet in the Snowflake console, or using SnowSQL):\n\n"
  },
  {
   "cell_type": "code",
   "id": "3a155280-358b-4bd4-abea-4158abd6fadc",
   "metadata": {
    "language": "sql",
    "name": "cell2"
   },
   "outputs": [],
   "source": "USE ROLE ACCOUNTADMIN;\nCREATE DATABASE IF NOT EXISTS API;\nCREATE WAREHOUSE IF NOT EXISTS DATA_API_WH WITH WAREHOUSE_SIZE='xsmall';",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7d7d7c8d-91b0-4af7-8530-0172b6c5028c",
   "metadata": {
    "name": "cell3",
    "collapsed": false
   },
   "source": "### Create the Application Role in Snowflake\n\nThe application will run as a new role with minimal priviledges. The following commands create the role and grant it access to the data needed for the application.\nRun the following commands in the Snowflake (in a cell in a Snowflake Notebook, in a Worksheet in the Snowflake console, or using SnowSQL):"
  },
  {
   "cell_type": "code",
   "id": "81068d08-40db-4236-b0c4-c356c7d9f539",
   "metadata": {
    "language": "sql",
    "name": "cell4"
   },
   "outputs": [],
   "source": "USE ROLE ACCOUNTADMIN;\nCREATE ROLE IF NOT EXISTS DATA_API_ROLE;\n\nGRANT ALL ON DATABASE API TO ROLE DATA_API_ROLE;\nGRANT ALL ON SCHEMA API.PUBLIC TO ROLE DATA_API_ROLE;\nGRANT USAGE ON WAREHOUSE DATA_API_WH TO ROLE DATA_API_ROLE;\nGRANT IMPORTED PRIVILEGES ON DATABASE SNOWFLAKE_SAMPLE_DATA TO ROLE DATA_API_ROLE;\n\nGRANT ROLE DATA_API_ROLE TO ROLE ACCOUNTADMIN;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f7482539-6534-4f41-b270-6c5f3d29cac0",
   "metadata": {
    "name": "Creating_the_Image_Registry",
    "collapsed": false
   },
   "source": "---\n## 6. Creating the Image Registry\n\nTo create the image registry, run the following commands:"
  },
  {
   "cell_type": "code",
   "id": "24a6df7d-10d4-4820-a910-36057362dc77",
   "metadata": {
    "language": "sql",
    "name": "cell6"
   },
   "outputs": [],
   "source": "USE ROLE ACCOUNTADMIN;\nUSE DATABASE API;\nCREATE OR REPLACE IMAGE REPOSITORY API;\n\nGRANT READ ON IMAGE REPOSITORY API TO ROLE DATA_API_ROLE;\n\nSHOW IMAGE REPOSITORIES;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "22545624-ab59-4074-9068-5c6d15bf1383",
   "metadata": {
    "name": "cell7",
    "collapsed": false
   },
   "source": "Note the `repository_url` in the response as that will be needed in the next step."
  },
  {
   "cell_type": "markdown",
   "id": "22f2dc7b-da85-47a1-9268-0e5b7fd431cf",
   "metadata": {
    "name": "Building_the_Application_Container",
    "collapsed": false
   },
   "source": "---\n## 7. Building the Application Container\n\nThe commands in this step are to be run in a terminal in Codespaces.\n\nTo create the application container, we will leverage docker. The Dockerfile is based on python 3.8 and installs the required libraries needed for \nthe application as well as the code. To create the docker container, run this command in the terminal provided by Codespaces:\n\n```bash\ndocker build -t dataapi .\n```\n\nNext, we need to tag the Docker image and push it to the image repository. To do so, replace the `<repository_url>` with the `repository_url` value \nreturned by the `SHOW IMAGE REPOSITORIES` command you ran above.\n\n```bash\ndocker tag dataapi <repository_url>/dataapi\n```\n\n```bash\ndocker push <repository_url>/dataapi\n```"
  },
  {
   "cell_type": "markdown",
   "id": "8c0b7f3e-f66b-42bb-b652-74d3b84a79a8",
   "metadata": {
    "name": "Creating_the_Compute_Pool",
    "collapsed": false
   },
   "source": "---\n## 8. Creating the Compute Pool\n\nTo create the compute pool to run the application, run the following commands:"
  },
  {
   "cell_type": "code",
   "id": "cb93830a-973c-4953-9cd5-d520967a79d2",
   "metadata": {
    "language": "sql",
    "name": "cell10"
   },
   "outputs": [],
   "source": "USE ROLE ACCOUNTADMIN;\n\nCREATE COMPUTE POOL API\n  MIN_NODES = 1\n  MAX_NODES = 5\n  INSTANCE_FAMILY = CPU_X64_XS;\n\nGRANT USAGE ON COMPUTE POOL API TO ROLE DATA_API_ROLE;\nGRANT MONITOR ON COMPUTE POOL API TO ROLE DATA_API_ROLE;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "43408d54-2aad-474c-8f7e-e46abccdb6bc",
   "metadata": {
    "name": "cell11",
    "collapsed": false
   },
   "source": "You can see the status of the `API` compute pool by running:"
  },
  {
   "cell_type": "code",
   "id": "babc43b1-a07f-4957-89c0-bd7974c4f4e7",
   "metadata": {
    "language": "sql",
    "name": "cell12"
   },
   "outputs": [],
   "source": "SHOW COMPUTE POOLS;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2c56a249-e3c3-4e3f-a487-9682660c9e9a",
   "metadata": {
    "name": "Creating_the_Application_Service",
    "collapsed": false
   },
   "source": "---\n## 9. Creating the Application Service\n\nTo create the service to host the application, run the following commands in the Snowflake (in a cell in a Snowflake Notebook, in a Worksheet in the Snowflake console, or using SnowSQL):"
  },
  {
   "cell_type": "code",
   "id": "ea5d3045-e4b3-453d-a92e-566e413d5d86",
   "metadata": {
    "language": "sql",
    "name": "cell14"
   },
   "outputs": [],
   "source": "USE ROLE ACCOUNTADMIN;\nGRANT BIND SERVICE ENDPOINT ON ACCOUNT TO ROLE DATA_API_ROLE;\n\nUSE ROLE DATA_API_ROLE;\nCREATE SERVICE API.PUBLIC.API\n IN COMPUTE POOL API\n FROM SPECIFICATION  \n$$\nspec:\n  container:\n  - name: api\n    image: /api/public/api/dataapi:latest\n    resources:                          \n      requests:\n        cpu: 0.5\n        memory: 128M\n      limits:\n        cpu: 1\n        memory: 256M\n  endpoint:\n  - name: api\n    port: 8001\n    public: true\nserviceRoles:\n- name: api_sr\n  endpoints:\n  - api\n$$\nQUERY_WAREHOUSE = DATA_API_WH;\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "aa7c70ae-31f5-410c-bb49-85ba1d22caf2",
   "metadata": {
    "name": "cell15",
    "collapsed": false
   },
   "source": "It will take a few minutes for your service to initialize, you can check status with these commands:"
  },
  {
   "cell_type": "code",
   "id": "ecb4a5f1-1125-419e-899d-0c1faac27536",
   "metadata": {
    "language": "sql",
    "name": "cell16"
   },
   "outputs": [],
   "source": "SHOW SERVICES IN COMPUTE POOL API;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7c12fe55-b67f-4a0b-a6da-da3b76607a86",
   "metadata": {
    "language": "sql",
    "name": "cell17"
   },
   "outputs": [],
   "source": "CALL SYSTEM$GET_SERVICE_STATUS('api.public.api');",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "08e3d1c8-4d8d-45b3-8a48-06fb43d44df1",
   "metadata": {
    "language": "sql",
    "name": "cell18"
   },
   "outputs": [],
   "source": "CALL SYSTEM$GET_SERVICE_LOGS('api.public.api', 0, 'api');",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6cb4c179-4a52-4005-ac48-4effe02bb28c",
   "metadata": {
    "name": "cell19",
    "collapsed": false
   },
   "source": "After your service has started, you can get the endpoints with the following command. Note that provisioning endpoints\ncan take a moment. While it does you will get a note like `Endpoints provisioning in progress...`:"
  },
  {
   "cell_type": "code",
   "id": "0730ce74-bab4-48c1-8b2d-c671401c81a5",
   "metadata": {
    "language": "sql",
    "name": "cell20"
   },
   "outputs": [],
   "source": "SHOW ENDPOINTS IN SERVICE API.PUBLIC.API;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3ec6ed33-e09b-4271-94e8-b12edd6c7a8f",
   "metadata": {
    "name": "cell21",
    "collapsed": false
   },
   "source": "Make note of the ingress_url as that will be need to test the application. This service will start the API, running at `https://<INGRESS_URL>`."
  },
  {
   "cell_type": "markdown",
   "id": "ec88b4de-2635-4486-9b08-dbcba29357a8",
   "metadata": {
    "name": "Testing_the_API",
    "collapsed": false
   },
   "source": "---\n## 10. Testing the API\n\nTo verify the API is online, go to the `https://<INGRESS_URL>` in your browser. You will be asked to authenticate to Snowflake and be given the root content: \n\n```json\n{\"result\":\"Nothing to see here\"}\n```\n\n### Endpoints\nThis API was implemented using both the Snowflake Python Connector and the Snowflake Snowpark for Python package. \nThey both implement the same API routes. The ones implemented with the Snowflke Python Connector are under the `/connector/` route.\nThe ones implemented with Snowpark Python are under the `/snowpark/` route.\n\n#### Top 10 Customers\nTo retrieve the top 10 customers in the date range of `1995-02-01` to `1995-02-14` using the Snowflake Connector for Python, use:\n\n```\nhttps://<SPCS_ENDPOINT_URL>/connector/customers/top10?start_range=1995-02-01&end_range=1995-02-14\n```\n\nTo retrieve the top 10 customers in the date range of `1995-02-01` to `1995-02-14` using the Snowflake Snowpark API, use:\n```\nhttps://<SPCS_ENDPOINT_URL>/snowpark/customers/top10?start_range=1995-02-01&end_range=1995-02-14\n```\n\nIf you call the endpoint without specifying the `start_range` then `1995-01-01` will be used. If you call the endpoint without specifying the `end_range` then `1995-03-31` will be used.\n\n#### Monthly sales for a given year for a sales clerk\nTo retrieve the monthly sales for clerk `000000002` for the year `1995` using the Snowflake Connector for Python, run:\n```\nhttps://<SPCS_ENDPOINT_URL>/connector/clerk/000000002/yearly_sales/1995\n```\n\nTo retrieve the monthly sales for clerk `000000002` for the year `1995` using the Snowflake Snowpark API, run:\n```\nhttps://<SPCS_ENDPOINT_URL>/snowpark/clerk/000000002/yearly_sales/1995\n```\n\n### Testing using a webpage\nThis project comes with a simple webpage that allows you to test the API. To get to it, open `https://<INGRESS_URL>/test` in a web browser.\n\nAt the top you can choose if you want to exercise the Snowflake Connector for Python or the Snowflake Snowpark API endpoints.\n\nThere are 2 forms below that. The first one allows you to enter parameters to test the \"Top 10 Customers\" endpoint. \nThe second one allows you to enter parameters to test the \"Monthly Clerk Sales\" endpoint.\n\nWhen you hit the `Submit` button, the API endpoint is called and the data is returned to the web page.\n\n"
  },
  {
   "cell_type": "markdown",
   "id": "5585b2e0-48e1-440e-b2f7-aea1a75856aa",
   "metadata": {
    "name": "Programmatic_Access",
    "collapsed": false
   },
   "source": "---\n## 11. Programmatic Access\n\nIn many situations we want to access this data API from another process outside of Snowflake programmatically. To do this, we will need a way to programmatically \nauthenticate to Snowflake to allow access to the SPCS endpoint. There are a number of ways to do this today, but we will focus on using \nProgrammatic Access Tokens (PAT), one of the simpler ways. \n\n\nRegardless of the authenictation method, the best practice is to create a user specifically for accessing this API endpoint, as well as a role for that user. \nTo do this, run the following commands:"
  },
  {
   "cell_type": "code",
   "id": "31f64aaa-f2e6-4d1f-a5ef-fd3e7a420c96",
   "metadata": {
    "language": "sql",
    "name": "cell24"
   },
   "outputs": [],
   "source": "USE ROLE ACCOUNTADMIN;\nCREATE ROLE IF NOT EXISTS APIROLE;\nGRANT ROLE APIROLE TO ROLE ACCOUNTADMIN;\nGRANT USAGE ON DATABASE API TO ROLE APIROLE;\nGRANT USAGE ON SCHEMA API.PUBLIC TO ROLE APIROLE;\nCREATE USER IF NOT EXISTS APIUSER PASSWORD='User123' DEFAULT_ROLE = apirole DEFAULT_SECONDARY_ROLES = ('ALL') MUST_CHANGE_PASSWORD = FALSE;\nGRANT ROLE APIROLE TO USER APIUSER;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2b5dd008-99e6-4933-8132-9cc9893b6947",
   "metadata": {
    "name": "cell25",
    "collapsed": false
   },
   "source": "Next, we can grant the service role to access the endpoint to the APIROLE we just created:"
  },
  {
   "cell_type": "code",
   "id": "3f080085-2a0b-42ef-b6e4-21f29340cc7b",
   "metadata": {
    "language": "sql",
    "name": "cell26"
   },
   "outputs": [],
   "source": "USE ROLE ACCOUNTADMIN;\nGRANT SERVICE ROLE api.public.api!api_sr TO ROLE apirole;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7e025219-1354-4232-a358-cf1cfb161e70",
   "metadata": {
    "name": "cell27",
    "collapsed": false
   },
   "source": "### Generating a PAT token via SQL\nLastly, we need to create a Programmatic Access Token for the `APIUSER`.\nIn order to use PAT, the user must have a network policy applied, so we create an \"allow-all\" network policy for this user. In practice you would limit to the\nIP/hostname origins for your clients.\n\nYou can do this via SQL as follows:"
  },
  {
   "cell_type": "code",
   "id": "8796136e-19e3-4125-a418-318cd1560e8c",
   "metadata": {
    "language": "sql",
    "name": "cell28"
   },
   "outputs": [],
   "source": "USE ROLE ACCOUNTADMIN;\nCREATE NETWORK POLICY IF NOT EXISTS api_np ALLOWED_IP_LIST = ('0.0.0.0/0');\nALTER USER apiuser SET NETWORK_POLICY = api_np;\nALTER USER IF EXISTS apiuser ADD PROGRAMMATIC ACCESS TOKEN api_token;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "55ac46cf-2163-490c-b47e-f9c9cc8f420b",
   "metadata": {
    "name": "cell29",
    "collapsed": false
   },
   "source": "Copy this PAT token and save it to a file. Save it to a file named `apiuser-token-secret.txt` in the `test/` directory of the cloned/downloaded repo."
  },
  {
   "cell_type": "markdown",
   "id": "11c56365-3c1f-4e7d-9aff-e55a7e72aac7",
   "metadata": {
    "name": "cell30",
    "collapsed": false
   },
   "source": "### Generating a PAT token via Snowsight\nAlternatively, you can use Snowsight to create the PAT token. Click on the \"Admin\" option on the sidebar, then the \"Users & Roles\" option in the sidebar.\nNext, click on the APIUSER user, and scroll down to the \"Programmatic access tokens\" section. Click the \"Generate new token\" button, give the token a name\n(such as `api_token`), choose the role `APIROLE` from the pull-down, leave the rest of the defaults, and click \"Generate\". Click the \"Download token\" button\nand save the file to the `test/` directory (you can leave the default filename)."
  },
  {
   "cell_type": "markdown",
   "id": "bed5fe5a-7b98-474b-bb07-602e4b02e821",
   "metadata": {
    "name": "Testing_Programmatically",
    "collapsed": false
   },
   "source": "---\n## 12. Testing Programmatically\n\nNext we can test accessing our API programmatically using the PAT token. We cannot use the PAT token directly to access the SPCS endpoint.\nWe must exchange the long-lived PAT token for a shorter-lived access token via the `/oauth/token` Snowflake endpoint. We can then use that\nshorter-lived token to access the SPCS endpoint. \n\nWe have 2 applications that demonstrate how to do this.\n\n### Accessing the endpoint via command-line program\nChange to the `test/` directory. In there is a program named `test.py`. You can see the usage instructions by running:\n\n```bash\npython test.py --help\n```\n\nYou must supply the following:\n* `ACCOUNT_URL` - this is the URL for your Snowflake account. It should be of the form `<ORGNAME>-<ACCTNAME>.snowflakecomputing.com`. You can find this in the\n  Snowflake console by clicking the circle with initials in the lower left and choosing the \"Connect a tool to Snowflake\" menu option. Copy the field\n  named \"Account/Server URL\".\n* `ROLE` - the role to use when accessing the endpoint. For this example, it should be `APIROLE`.\n* `ENDPOINT` - this is the full URL you are trying to access. E.g., `https://<HASH>-<ORGNAME>-<ACCTNAME>.snowflakecomputing.app/connector/customers/top10`\n\nThere are 3 ways to specify the PAT to use:\n1. Use the `--pat` option and supply the full PAT token. E.g., `--pat <PAT>`.\n2. Use the `--patfile` option and supply the filename to the PAT token file. E.g., `--patfile /path/to/patfile`\n3. If you saved your PAT token to this directory, and it ends with `-token-secret.txt` the application will discover it and use it.\n\nFor example, your call might look something like:\n\n```bash\npython test.py --account_url 'MYORG-MYACCT.snowflakecomputing.com' --role APIROLE --endpoint 'https://mzbqa5c-myorg-myacct.snowflakecomputing.app/connector/customers/top10'\n```"
  },
  {
   "cell_type": "markdown",
   "id": "2a82ad4f-44f3-4e26-99a5-4333e44f804f",
   "metadata": {
    "name": "cell32",
    "collapsed": false
   },
   "source": "### Accessing the endpoint via Streamlit\nThe repository also contains a Streamlit to access the endpoint. \n\nTo use Streamlit we must first install the Streamlit library:\n\n```bash\npip install streamlit\n```\n\nNext, change to the `test/` directory, and run:\n\n```bash\npython -m streamlit run test_streamlit.py\n```\n\nEnter the account URL, role, and URL in the supplied boxes.\n\nThe Streamlit will attempt to detect the PAT in the local directory in a file ending with `-token-secret.txt`. If one is found, it will use that as the PAT. \nIf not, it will show another box to enter the PAT in (the actual PAT value, not the filename).\n\nEnter the necessary items and click \"Fetch it!\". You will get a status update that it is \"Trading PAT for Token...\" and then \"Getting data...\" and then it will\ndisplay the result from SPCS."
  },
  {
   "cell_type": "markdown",
   "id": "f2cff554-68ad-4c3b-81f7-b0d962e36ac1",
   "metadata": {
    "name": "Stopping_the_API",
    "collapsed": false
   },
   "source": "---\n## 13. Stopping the API\n\nTo stop the API, you can suspend the service. \nRun the following commands:\n"
  },
  {
   "cell_type": "code",
   "id": "5b0e0747-85a0-4997-91fa-a4e5606b6a93",
   "metadata": {
    "language": "sql",
    "name": "cell34"
   },
   "outputs": [],
   "source": "USE ROLE DATA_API_ROLE;\nALTER SERVICE API.PUBLIC.API SUSPEND;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "675e5997-08a9-4e9f-97e2-1ffb23c3ea37",
   "metadata": {
    "name": "Cleanup",
    "collapsed": false
   },
   "source": "## 14. Cleanup\n\nTo fully remove everything you did today you only need to drop some objects in your Snowflake account. \nRun the following commands:"
  },
  {
   "cell_type": "code",
   "id": "94eeff63-7922-4c78-b30b-183a310e2c52",
   "metadata": {
    "language": "sql",
    "name": "cell36"
   },
   "outputs": [],
   "source": "USE ROLE ACCOUNTADMIN;\n\nDROP DATABASE IF EXISTS API;\nDROP ROLE IF EXISTS DATA_API_ROLE;\nDROP COMPUTE POOL IF EXISTS API;\nDROP WAREHOUSE IF EXISTS DATA_API_WH;\nDROP USER IF EXISTS APIUSER;\nDROP ROLE IF EXISTS APIROLE;\nDROP NETWORK POLICY api_np;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1130ca6b-f043-4cc3-bc8c-5c28f7cd7931",
   "metadata": {
    "name": "Conclusion",
    "collapsed": false
   },
   "source": "## 15. Conclusion\n\nYou've successfully built a custom API in Python Powered by Snowflake. \n\nWhen you go to put a data API into production you should protect the API with some level of authentication and authorization. You can do this with ngrok, but you will need to change the configuration used as this example had it publicly accessible. \n\nAnother consideration is enabling a frontend website to access the endpoint, the test site worked in this example because it's hosted on the same domain as the api. If you need to access the api from another website, you will need to do additional configuration.\n\nTo get more comfortable with this solution, implement new endpoints pointing to the sample dataset provided or other datasets.\n\nCode for this project is available at [https://github.com/sfc-gh-bhess/lab_data_api_python](https://github.com/sfc-gh-bhess/lab_data_api_python).\n\n### What we've covered\n- How to configure and build a custom API Powered by Snowflake\n- How to run and test the API on your machine\n"
  }
 ]
}